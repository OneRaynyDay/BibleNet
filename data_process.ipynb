{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Imports and Constants#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Imports for the project #\n",
    "###########################\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import h5py\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Constants #\n",
    "#############\n",
    "\n",
    "word_sequence_dest = \"word_sequence.hdf5\"\n",
    "word_mapping_dest = \"word_map.pkl\"\n",
    "idx_mapping_dest = \"idx_map.pkl\"\n",
    "delims = ' |\\t|\\n|\\r\\n|:'\n",
    "prune_freq = 1 # the word must appear >5 times in the entire text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Acquire dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already the .txt's of the bible in our own file directory. All we need to do is to read it. \n",
    "\n",
    "**Currently we are only reading in the English version.**\n",
    "\n",
    "**TODO: We don't have a \"proper\" tokenizer right now. We are just delimiting via spaces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"overfit.txt\") as f:\n",
    "    bible = re.split(delims, f.read())\n",
    "\n",
    "# remove white space:\n",
    "bible = [word for word in bible if word != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All', 'Speak,', 'speak.', 'First', 'Citizen', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', 'All', 'Resolved.', 'resolved.', 'First', 'Citizen', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', 'All', 'We', \"know't,\", 'we', \"know't.\", 'First', 'Citizen', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', \"Is't\", 'a', 'verdict?', 'All', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done', 'away,', 'away!', 'Second', 'Citizen', 'One', 'word,', 'good', 'citizens.', 'First', 'Citizen', 'We', 'are', 'accounted', 'poor', 'citizens,', 'the', 'patricians', 'good.', 'What', 'authority', 'surfeits', 'on', 'would', 'relieve', 'us', 'if', 'they', 'would']\n"
     ]
    }
   ],
   "source": [
    "bible_map_word_to_freq = {}\n",
    "\n",
    "for word in bible:\n",
    "    if word in bible_map_word_to_freq:\n",
    "        bible_map_word_to_freq[word] += 1\n",
    "    else:\n",
    "        bible_map_word_to_freq[word] = 1\n",
    "\n",
    "bible = [\"<unknown>\" if bible_map_word_to_freq[word] < prune_freq else word for word in bible]\n",
    "print bible[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change bible into an np array. In this case it has around 900k words. Therefore, it will be a (900k,) array.\n",
    "bible_seq = np.array(bible)\n",
    "\n",
    "bible_set = set(bible)\n",
    "bible_map_word_to_idx = {word : i for i, word in enumerate(bible_set)} # We have about 30k unique vocabularies.\n",
    "bible_map_idx_to_word = {i : word for i, word in enumerate(bible_set)} # We have about 30k unique vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850\n",
      "1850\n",
      "1849\n"
     ]
    }
   ],
   "source": [
    "print len(bible_map_idx_to_word)\n",
    "print len(bible_set)\n",
    "max_val = 0\n",
    "for key in bible_map_word_to_idx:\n",
    "    max_val = max(max_val, bible_map_word_to_idx[key])\n",
    "print max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a numpy array, so we use h5py.\n",
    "with h5py.File(word_sequence_dest, 'w') as f:\n",
    "    f.create_dataset('bible_seq', data=bible_seq)\n",
    "# This is a python dict, so we use pickle.\n",
    "with open(word_mapping_dest, 'w') as f:\n",
    "    pickle.dump(bible_map_word_to_idx, f)\n",
    "with open(idx_mapping_dest, 'w') as f:\n",
    "    pickle.dump(bible_map_idx_to_word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
