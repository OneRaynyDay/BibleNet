{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Imports and Constants#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Imports for the project #\n",
    "###########################\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import h5py\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Constants #\n",
    "#############\n",
    "\n",
    "word_sequence_dest = \"word_sequence.hdf5\"\n",
    "word_mapping_dest = \"word_map.pkl\"\n",
    "delims = ' |\\t|\\r\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Acquire dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already the .txt's of the bible in our own file directory. All we need to do is to read it. \n",
    "\n",
    "**Currently we are only reading in the English version.**\n",
    "\n",
    "**TODO: We don't have a \"proper\" tokenizer right now. We are just delimiting via spaces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"bible.txt\") as f:\n",
    "    bible = re.split(delims, f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change bible into an np array. In this case it has around 900k words. Therefore, it will be a (900k,) array.\n",
    "bible_seq = np.array(bible)\n",
    "bible_set = set(bible)\n",
    "bible_map = {word : i for i, word in enumerate(bible_set)} # We have about 30k unique vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a numpy array, so we use h5py.\n",
    "with h5py.File(word_sequence_dest, 'w') as f:\n",
    "    f.create_dataset('bible_seq', data=bible_seq)\n",
    "# This is a python dict, so we use pickle.\n",
    "with open(word_mapping_dest, 'w') as f:\n",
    "    pickle.dump(bible_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
