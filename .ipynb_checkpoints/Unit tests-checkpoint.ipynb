{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "from models.networks.vanilla_rnn import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try out some simple functions for numerical_gradient. #\n",
    "\n",
    "We know the linear equation y = 3x should always return 3. Let's check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99999999989\n"
     ]
    }
   ],
   "source": [
    "def linear(x, slope=3):\n",
    "    return slope*x\n",
    "\n",
    "slope = numerical_gradient_check_scalar(linear, 5)\n",
    "print slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx_num :  [[  8.99999977   9.99999975  10.99999972]\n",
      " [  8.99999977  11.9999997   14.99999962]]\n",
      "dy_num :  [[ 2.99999992  3.9999999   4.99999987]\n",
      " [ 0.99999997  1.99999995  2.99999992]]\n"
     ]
    }
   ],
   "source": [
    "# Multi-argument function that passes back two streams of gradients.\n",
    "# this is a sanity pre-check for LSTM's.\n",
    "def multi(x,y):\n",
    "    a = x*y\n",
    "    b = x**2\n",
    "    return a,b\n",
    "\n",
    "multi_x_a = lambda x: multi(x,y)[0]\n",
    "multi_y_a = lambda y: multi(x,y)[0]\n",
    "multi_x_b = lambda x: multi(x,y)[1]\n",
    "multi_y_b = lambda y: multi(x,y)[1]\n",
    "\n",
    "x,y = np.array([[3,4,5],[1,2,3]], float), np.array([[3,2,1],[7,8,9]], float)\n",
    "a,b = multi(x,y)\n",
    "# dummy gradients\n",
    "da = np.ones_like(a)\n",
    "db = np.ones_like(b)\n",
    "\n",
    "dx_num = numerical_gradient_check_multivar(multi_x_a, x, da) + numerical_gradient_check_multivar(multi_x_b, x, db)\n",
    "dy_num = numerical_gradient_check_multivar(multi_y_a, y, da) + numerical_gradient_check_multivar(multi_y_b, y, db)\n",
    "\n",
    "print \"dx_num : \", dx_num\n",
    "print \"dy_num : \", dy_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29.03938293   8.09431076]\n",
      "[[ 8.01086426  3.0040741 ]\n",
      " [ 8.01086426  3.0040741 ]]\n"
     ]
    }
   ],
   "source": [
    "def multi_cubic_field(x):\n",
    "    return np.array([x[0]**3 + x[1]**2, x[0]*2 + x[1]/12])\n",
    "arr = np.array([3,4], dtype=np.float32)\n",
    "\n",
    "def matrix_mult(x, b = np.array([[3,5],[2,1]])):\n",
    "    return x.dot(b)\n",
    "                \n",
    "vector_field = numerical_gradient_check_multivar(multi_cubic_field, arr, np.ones_like(arr))\n",
    "print vector_field\n",
    "\n",
    "arr = np.array([[3,4],[1,2]], dtype=np.float32)\n",
    "vector_field = numerical_gradient_check_multivar(matrix_mult, arr, np.ones_like(arr))\n",
    "print vector_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fw :  [[ 0.99999997  0.99999997  0.99999997]\n",
      " [ 1.99999995  1.99999995  1.99999995]]\n",
      "fx :  [[ 5.99999985  7.9999998 ]]\n",
      "fb :  [[ 0.99999997  0.99999997  0.99999997]]\n"
     ]
    }
   ],
   "source": [
    "def affine_transform(w, x, b):\n",
    "    return x.dot(w) + b\n",
    "\n",
    "x = np.array([[1,2]], float) # 1 x 2\n",
    "w = np.array([[3,2,1],[1,2,5]], float) # 2 x 3\n",
    "b = np.array([[1,5,7]], float) # 1 x 3\n",
    "dummy = affine_transform(w,x,b)\n",
    "\n",
    "fw = lambda w: affine_transform(w,x,b)\n",
    "fx = lambda x: affine_transform(w,x,b)\n",
    "fb = lambda b: affine_transform(w,x,b)\n",
    "\n",
    "vector_field = numerical_gradient_check_multivar(fw, w, np.ones_like(dummy))\n",
    "print \"fw : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fx, x, np.ones_like(dummy))\n",
    "print \"fx : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fb, b, np.ones_like(dummy))\n",
    "print \"fb : \", vector_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do a linear regression check (OLS) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def costFunction(X, Theta, y):\n",
    "    '''\n",
    "    m: # of samples\n",
    "    :return: COST of current theta\n",
    "    '''\n",
    "    h = np.dot(X, Theta)\n",
    "    cost = np.sum((h - y)**2) / 2 / h.shape[0]\n",
    "    delta_weight = np.dot(X.T, h - y) / (X.shape[0])\n",
    "    return cost, delta_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60761655]\n",
      " [ 0.48703559]\n",
      " [ 0.56804149]\n",
      " [ 0.53638075]\n",
      " [ 0.59080817]]\n",
      "[[ 0.60761656]\n",
      " [ 0.4870356 ]\n",
      " [ 0.56804151]\n",
      " [ 0.53638077]\n",
      " [ 0.59080819]]\n",
      "1.26323844376e-08\n"
     ]
    }
   ],
   "source": [
    "# Say we have 5 dimensions on X and 1 on Y, we have N = 10\n",
    "N, X_size, Y_size = 10, 5, 1\n",
    "\n",
    "X = np.random.random((N, X_size))\n",
    "Y = np.random.random((N, Y_size))\n",
    "Theta = np.random.random((X_size,1))\n",
    "fTheta = lambda Theta: costFunction(X, Theta, Y)[0]\n",
    "\n",
    "dTheta_num = numerical_gradient_check_multivar(fTheta, Theta, 1)\n",
    "_, dTheta = costFunction(X, Theta, Y)\n",
    "\n",
    "print dTheta_num\n",
    "print dTheta\n",
    "print norm_loss(dTheta_num, dTheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word_embedding_forward/backward #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 4) \n",
      "[[[3 4 7 1]\n",
      "  [3 4 7 1]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[1 5 9 4]\n",
      "  [1 5 9 4]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[4 3 2 5]\n",
      "  [3 4 7 1]\n",
      "  [4 3 2 5]]]\n"
     ]
    }
   ],
   "source": [
    "# Looks good to me\n",
    "ans = np.array([[[3, 4, 7, 1],\n",
    "                [3, 4, 7, 1],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[1, 5, 9, 4],\n",
    "                [1, 5, 9, 4],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[4, 3, 2, 5],\n",
    "                [3, 4, 7, 1],\n",
    "                [4, 3, 2, 5]]])\n",
    "\n",
    "x = np.array([[1,1,0], [0,0,0], [2,1,2]], int)\n",
    "words = np.array([[1,5,9,4],[3,4,7,1],[4,3,2,5]])\n",
    "arr = word_embedding_forward(words, x)\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) \n",
      "[[  7.  20.  13.  11.]\n",
      " [  4.   5.   9.   7.]\n",
      " [  4.   0.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "dout = np.array([[[1,2,0,1],[3,2,9,1],[1,2,1,1]],\n",
    "                 [[3,9,2,4],[1,9,9,0],[2,0,1,6]],\n",
    "                 [[1,0,1,0],[0,1,0,5],[3,0,0,1]]])\n",
    "\n",
    "arr = word_embedding_backward(dout, words, x)\n",
    "ans = np.array([[  7.,  20.,  13.,  11.],\n",
    "               [  4.,   5.,   9.,   7.],\n",
    "               [  4.,   0.,   1.,   1.]])\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN_step Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "prev_h = (N,H)\n",
    "x = (N, V)\n",
    "W_hh = (H,H)\n",
    "W_xh = (V,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "N = 10\n",
    "D = 3\n",
    "H = 2\n",
    "T = 1\n",
    "V = 5\n",
    "\n",
    "prev_h = np.random.random((N,H)) # N = 3, H = 5\n",
    "x = np.random.random((N,V)) # N = 3, V = 4\n",
    "W_hh = np.random.random((H,H)) # H = 5\n",
    "W_xh = np.random.random((V,H)) # V = 4, H = 5\n",
    "b = np.random.random((H,)) # H = 5\n",
    "\n",
    "res = rnn_step_forward(prev_h, W_hh, x, W_xh, b) # N = 3, H = 5\n",
    "gradients = np.random.random(res.shape)\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev_h :  1.26320346526e-08\n",
      "dW_hh :  1.26243358504e-08\n",
      "dx :  1.26243329418e-08\n",
      "dW_xh :  1.26241828239e-08\n",
      "db :  1.26147634111e-08\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fprev_h = lambda prev_h: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_hh = lambda W_hh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fx = lambda x: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_xh = lambda W_xh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fb = lambda b: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "\n",
    "dprev_h_num = numerical_gradient_check_multivar(fprev_h, prev_h, gradients)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh, gradients)\n",
    "dx_num = numerical_gradient_check_multivar(fx, x, gradients)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh, gradients)\n",
    "db_num = numerical_gradient_check_multivar(fb, b, gradients)\n",
    "\n",
    "dW_hh, dW_xh, dprev_h, dx, db = rnn_step_backward(prev_h, W_hh, x, W_xh, b, gradients)\n",
    "print \"dprev_h : \", norm_loss(dprev_h, dprev_h_num)\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/layers.py:282: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if h0 != None: # Supply an h0 state.\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "h0 = (N,H)\n",
    "W_hh = (H,H)\n",
    "x = (N,T,D)\n",
    "W_xh = (D,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "N = 10\n",
    "D = 3\n",
    "H = 2\n",
    "T = 1\n",
    "\n",
    "h0 = np.random.random((N,H)) * 2\n",
    "W_hh = np.random.random((H,H)) * 2\n",
    "x = np.random.random((N,T,D)) * 2\n",
    "W_xh = np.random.random((D,H)) * 2\n",
    "b = np.zeros((H,))\n",
    "x_copy = x.copy()\n",
    "\n",
    "h = rnn_forward(x, W_xh, W_hh, b, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0042479   0.00371023]\n",
      " [ 0.00221519  0.00078672]] [[ 0.0042479   0.00371023]\n",
      " [ 0.00221519  0.00078672]]\n",
      "[ 0.0030691   0.00233127] [ 0.0030691   0.00233127]\n",
      "dW_hh :  1.23252048922e-08\n",
      "dW_xh :  1.32962453723e-08\n",
      "dx :  1.26422152002e-08\n",
      "db :  1.28893202866e-08\n",
      "dh0 :  1.26197410925e-08\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fx = lambda x: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_xh = lambda W_xh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_hh = lambda W_hh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fb = lambda b: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fh0 = lambda h0: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "\n",
    "dx_num = numerical_gradient_check_multivar(fx, x, np.ones_like(h))\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh, np.ones_like(h))\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh, np.ones_like(h))\n",
    "db_num = numerical_gradient_check_multivar(fb, b, np.ones_like(h))\n",
    "dh0_num = numerical_gradient_check_multivar(fh0, h0, np.ones_like(h))\n",
    "\n",
    "dW_hh, dW_xh, dx, db, dh0 = rnn_backward(x, W_xh, W_hh, b, h0, h, np.ones_like(h))\n",
    "print dW_hh, dW_hh_num\n",
    "print db, db_num\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"db : \", norm_loss(db, db_num)\n",
    "print \"dh0 : \", norm_loss(dh0, dh0_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.58172089 -0.50182032 -0.41232771 -0.31410098]\n",
      " [ 0.66854692  0.79562378  0.87755553  0.92795967]\n",
      " [ 0.97934501  0.99144213  0.99646691  0.99854353]]\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "next_h = rnn_step_forward(prev_h, Wh, x, Wx, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print next_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monster LSTM_Step #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Forward pass \"\"\"\n",
    "\"\"\" Stealing one from the books \"\"\"\n",
    "N, V, H = 10,30,20\n",
    "x = np.linspace(-0.4, 1.2, num=N*V).reshape(N, V)\n",
    "prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)\n",
    "prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)\n",
    "W_xh = np.linspace(-2.1, 1.3, num=4*V*H).reshape(V, 4 * H)\n",
    "W_hh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.3, 0.7, num=4*H)\n",
    "\n",
    "cache, c, h = lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev_h :  1.2631887577e-08\n",
      "dW_hh :  1.26313967883e-08\n",
      "dx :  1.26331718809e-08\n",
      "dW_xh :  1.26318084358e-08\n",
      "db :  1.26278195781e-08\n",
      "dprev_c :  1.26321386015e-08\n"
     ]
    }
   ],
   "source": [
    "# Backwards!\n",
    "# So LSTM's are very complicated because they have 2 passes of gradients flowing backwards.\n",
    "# however, we know by the additive principal of gradients, that the gradients can simply be\n",
    "# added together to form the final gradient. Therefore, we will compute the gradient of c\n",
    "# then the gradient of h, and add the two together and it SHOULD be equal to our step_backwards.\n",
    "\n",
    "dc, dh = np.zeros(c.shape), np.random.random(h.shape)\n",
    "\n",
    "fprev_h_c = lambda prev_h: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "fW_hh_c = lambda W_hh: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "fx_c = lambda x: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "fW_xh_c = lambda W_xh: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "fb_c = lambda b: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "fprev_c_c = lambda prev_c: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[1]\n",
    "\n",
    "fprev_h_h = lambda prev_h: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "fW_hh_h = lambda W_hh: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "fx_h = lambda x: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "fW_xh_h = lambda W_xh: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "fb_h = lambda b: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "fprev_c_h = lambda prev_c: lstm_step_forward(prev_h, W_hh, x, W_xh, b, prev_c)[2]\n",
    "\n",
    "dprev_h_num = numerical_gradient_check_multivar(fprev_h_c, prev_h, dc) + \\\n",
    "              numerical_gradient_check_multivar(fprev_h_h, prev_h, dh)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh_c, W_hh, dc) + \\\n",
    "            numerical_gradient_check_multivar(fW_hh_h, W_hh, dh)\n",
    "dx_num = numerical_gradient_check_multivar(fx_c, x, dc) + \\\n",
    "         numerical_gradient_check_multivar(fx_h, x, dh)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh_c, W_xh, dc) + \\\n",
    "            numerical_gradient_check_multivar(fW_xh_h, W_xh, dh)\n",
    "db_num = numerical_gradient_check_multivar(fb_c, b, dc) + \\\n",
    "         numerical_gradient_check_multivar(fb_h, b, dh)\n",
    "dprev_c_num = numerical_gradient_check_multivar(fprev_c_c, prev_c, dc) + \\\n",
    "              numerical_gradient_check_multivar(fprev_c_h, prev_c, dh)\n",
    "\n",
    "dW_hh, dW_xh, dprev_h, dx, db, dprev_c = lstm_step_backward(W_hh, x, W_xh, b, cache, dh, dc)\n",
    "\n",
    "print \"dprev_h : \", norm_loss(dprev_h, dprev_h_num)\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"db : \", norm_loss(db, db_num)\n",
    "print \"dprev_c : \", norm_loss(dprev_c, dprev_c_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM pass #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "from models.networks.vanilla_rnn import *\n",
    "import numpy as np\n",
    "\n",
    "N, V, H, T = 3, 5, 4, 6\n",
    "x = np.linspace(-0.4, 0.6, num=N*T*V).reshape(N, T, V)\n",
    "h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)\n",
    "W_xh = np.linspace(-0.2, 0.9, num=4*V*H).reshape(V, 4 * H)\n",
    "W_hh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.2, 0.7, num=4*H)\n",
    "\n",
    "caches, h = lstm_forward(x, W_xh, W_hh, b, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_hh :  1.26256273091e-08\n",
      "dW_xh :  1.26401079848e-08\n",
      "dx :  1.26374131558e-08\n",
      "db :  1.26448931228e-08\n",
      "dh0 :  1.26217940386e-08\n"
     ]
    }
   ],
   "source": [
    "gradient = np.random.random(h.shape)\n",
    "\n",
    "fx = lambda x: lstm_forward(x, W_xh, W_hh, b, h0)[1]\n",
    "fW_xh = lambda W_xh: lstm_forward(x, W_xh, W_hh, b, h0)[1]\n",
    "fW_hh = lambda W_hh: lstm_forward(x, W_xh, W_hh, b, h0)[1]\n",
    "fb = lambda b: lstm_forward(x, W_xh, W_hh, b, h0)[1]\n",
    "fh0 = lambda h0: lstm_forward(x, W_xh, W_hh, b, h0)[1]\n",
    "\n",
    "dx_num = numerical_gradient_check_multivar(fx, x, gradient)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh, gradient)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh, gradient)\n",
    "db_num = numerical_gradient_check_multivar(fb, b, gradient)\n",
    "dh0_num = numerical_gradient_check_multivar(fh0, h0, gradient)\n",
    "\n",
    "dW_hh, dW_xh, dx, db, dh0 = lstm_backward(x, W_xh, W_hh, b, h0, caches, gradient)\n",
    "\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"db : \", norm_loss(db, db_num)\n",
    "print \"dh0 : \", norm_loss(dh0, dh0_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "h = (N,H)\n",
    "W_hy = (H,D)\n",
    "b = (D,)\n",
    "\"\"\"\n",
    "h = np.random.random((3,5)) # N = 3, H = 5\n",
    "W_hy = np.random.random((5,7)) # H = 5, D = 7\n",
    "b = np.random.random((7,)) # D = 7\n",
    "\n",
    "res = affine_forward(h, W_hy, b) # N = 3, D = 7\n",
    "gradients = np.random.random(res.shape)\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx :  1.26314633753e-08\n",
      "dW_xh :  1.26305748999e-08\n",
      "db :  1.26310626321e-08\n"
     ]
    }
   ],
   "source": [
    "fh = lambda h: affine_forward(h, W_hy, b)\n",
    "fW_hy = lambda W_hy: affine_forward(h, W_hy, b)\n",
    "fb = lambda b: affine_forward(h, W_hy, b)\n",
    "\n",
    "dh_num = numerical_gradient_check_multivar(fh, h,gradients)\n",
    "dW_hy_num = numerical_gradient_check_multivar(fW_hy, W_hy,gradients)\n",
    "db_num = numerical_gradient_check_multivar(fb, b,gradients)\n",
    "\n",
    "dh, dW_hy, db = affine_backward(h, W_hy, b, gradients)\n",
    "\n",
    "print \"dx : \", norm_loss(dh, dh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_hy, dW_hy_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer for RNN's #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "h = (N,T,H)\n",
    "W_hy = (H,D)\n",
    "b = (D,)\n",
    "\"\"\"\n",
    "h = np.random.random((3,7,5)) # N = 3, T = 7, H = 5\n",
    "W_hy = np.random.random((5,7)) # H = 5, D = 7\n",
    "b = np.random.random((7,)) # D = 7\n",
    "\n",
    "res = rnn_affine_forward(h, W_hy, b) # N = 3, D = 7\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx :  1.26308770636e-08\n",
      "dW_xh :  1.26311152626e-08\n",
      "db :  1.26310625571e-08\n"
     ]
    }
   ],
   "source": [
    "gradients = np.random.random(res.shape)\n",
    "\n",
    "fh = lambda h: rnn_affine_forward(h, W_hy, b)\n",
    "fW_hy = lambda W_hy: rnn_affine_forward(h, W_hy, b)\n",
    "fb = lambda b: rnn_affine_forward(h, W_hy, b)\n",
    "\n",
    "dh_num = numerical_gradient_check_multivar(fh, h, gradients)\n",
    "dW_hy_num = numerical_gradient_check_multivar(fW_hy, W_hy, gradients)\n",
    "db_num = numerical_gradient_check_multivar(fb, b, gradients)\n",
    "\n",
    "dh, dW_hy, db = rnn_affine_backward(h, W_hy, b, gradients)\n",
    "\n",
    "print \"dx : \", norm_loss(dh, dh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_hy, dW_hy_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Layer - One of the most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26331639816e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,4)) # N = 3, D = 4\n",
    "y = np.random.randint(4, size=3) # D = 4, N = 3\n",
    "\n",
    "fx = lambda x: softmax(x, y)[0]\n",
    "\n",
    "loss, dJ = softmax(x, y)\n",
    "\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x, 1)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Layer - The other most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26666177132e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,5)) # N = 3, D = 5\n",
    "y = np.random.randint(5, size=3) # D = 5, N = 3\n",
    "\n",
    "fx = lambda x: SVM(x, y)[0]\n",
    "\n",
    "loss, dJ = SVM(x, y)\n",
    "\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x, 1)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax for RNN layer - the important function compatible for RNN's #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26174307889e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,T,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,5,4)) # N = 3, T = 5, D = 4\n",
    "y = np.random.randint(4, size=(3,5)) # D = 4, N = 3\n",
    "\n",
    "fx = lambda x: rnn_softmax(x, y)[0]\n",
    "\n",
    "loss, dJ = rnn_softmax(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x, 1)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I needed regularization on my net so I dropped out of UCLA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000.0\n",
      "59820.0\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,T,D)\n",
    "\"\"\"\n",
    "x = np.ones((30,50,40), float) # N = 30, T = 50, D = 40\n",
    "# by def, 30x50x40 = sum of x = 60,000\n",
    "p = 0.6 # we should see a number around 36,000\n",
    "\n",
    "# Numerical checking on this kind of stuff is sketch since we have randomness in the function\n",
    "print np.sum(x)\n",
    "x,_ = dropout_forward(x, p)\n",
    "print np.sum(x)\n",
    "# These should be relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The entire RNN forward() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.42070749 -0.27279261 -0.11074945  0.05740409  0.22236251]\n",
      "  [-0.39525808 -0.22554661 -0.0409454   0.14649412  0.32397316]\n",
      "  [-0.42305111 -0.24223728 -0.04287027  0.15997045  0.35014525]]\n",
      "\n",
      " [[-0.55857474 -0.39065825 -0.19198182  0.02378408  0.23735671]\n",
      "  [-0.27150199 -0.07088804  0.13562939  0.33099728  0.50158768]\n",
      "  [-0.51014825 -0.30524429 -0.06755202  0.17806392  0.40333043]]]\n"
     ]
    }
   ],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h = rnn_forward(x, Wx, Wh, b, h0)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One pass #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_rnn\n",
      "(1,)\n",
      "W_xh\n",
      "(2, 1)\n",
      "W_hy\n",
      "(1, 2)\n",
      "words\n",
      "(2, 2)\n",
      "b_affine\n",
      "(2,)\n",
      "W_hh\n",
      "(1, 1)\n",
      "[[-1.5]]\n",
      "[[0 1]]\n",
      "0.00495557253145\n"
     ]
    }
   ],
   "source": [
    "N, D, V, H, T = 1,2,2,1,2\n",
    "\n",
    "model = VanillaRNN(N, D, T, H, V)\n",
    "\n",
    "np.random.seed(5)\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.iteritems():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "    print k\n",
    "    print model.params[k].shape\n",
    "\n",
    "h0 = np.linspace(-1.5, 0.3, num=(N * H)).reshape(N, H)\n",
    "captions = (np.arange(N * T) % V).reshape(N, T)\n",
    "print h0\n",
    "print captions\n",
    "x = captions[:,:-1]\n",
    "y = captions[:,1:]\n",
    "\n",
    "loss, grads, _ = model.loss(x, y, h0)\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
