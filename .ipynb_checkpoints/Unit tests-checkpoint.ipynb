{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "from models.networks.vanilla_rnn import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try out some simple functions for numerical_gradient. #\n",
    "\n",
    "We know the linear equation y = 3x should always return 3. Let's check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99999999989\n"
     ]
    }
   ],
   "source": [
    "def linear(x, slope=3):\n",
    "    return slope*x\n",
    "\n",
    "slope = numerical_gradient_check_scalar(linear, 5)\n",
    "print slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.9999999   0.99999997]\n"
     ]
    }
   ],
   "source": [
    "def multi_quadratic(x):\n",
    "    return x[0]**2 + x[1]\n",
    "arr = np.array([2,2], float)\n",
    "\n",
    "slope = numerical_gradient_check_multivar(multi_quadratic, arr)\n",
    "print slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29.03938293   8.09431076]\n",
      "[[ 8.01086426  3.0040741 ]\n",
      " [ 8.01086426  3.0040741 ]]\n"
     ]
    }
   ],
   "source": [
    "def multi_cubic_field(x):\n",
    "    return np.array([x[0]**3 + x[1]**2, x[0]*2 + x[1]/12])\n",
    "arr = np.array([3,4], dtype=np.float32)\n",
    "\n",
    "def matrix_mult(x, b = np.array([[3,5],[2,1]])):\n",
    "    return x.dot(b)\n",
    "                \n",
    "vector_field = numerical_gradient_check_multivar(multi_cubic_field, arr)\n",
    "print vector_field\n",
    "\n",
    "arr = np.array([[3,4],[1,2]], dtype=np.float32)\n",
    "vector_field = numerical_gradient_check_multivar(matrix_mult, arr)\n",
    "print vector_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fw :  [[ 0.99999997  0.99999997  0.99999997]\n",
      " [ 1.99999995  1.99999995  1.99999995]]\n",
      "fx :  [[ 5.99999985  7.9999998 ]]\n",
      "fb :  [[ 0.99999997  0.99999997  0.99999997]]\n"
     ]
    }
   ],
   "source": [
    "def affine_transform(w, x, b):\n",
    "    return x.dot(w) + b\n",
    "\n",
    "x = np.array([[1,2]], float) # 1 x 2\n",
    "w = np.array([[3,2,1],[1,2,5]], float) # 2 x 3\n",
    "b = np.array([[1,5,7]], float) # 1 x 3\n",
    "\n",
    "fw = lambda w: affine_transform(w,x,b)\n",
    "fx = lambda x: affine_transform(w,x,b)\n",
    "fb = lambda b: affine_transform(w,x,b)\n",
    "\n",
    "vector_field = numerical_gradient_check_multivar(fw, w)\n",
    "print \"fw : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fx, x)\n",
    "print \"fx : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fb, b)\n",
    "print \"fb : \", vector_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word_embedding_forward/backward #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 4) \n",
      "[[[3 4 7 1]\n",
      "  [3 4 7 1]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[1 5 9 4]\n",
      "  [1 5 9 4]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[4 3 2 5]\n",
      "  [3 4 7 1]\n",
      "  [4 3 2 5]]]\n"
     ]
    }
   ],
   "source": [
    "# Looks good to me\n",
    "ans = np.array([[[3, 4, 7, 1],\n",
    "                [3, 4, 7, 1],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[1, 5, 9, 4],\n",
    "                [1, 5, 9, 4],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[4, 3, 2, 5],\n",
    "                [3, 4, 7, 1],\n",
    "                [4, 3, 2, 5]]])\n",
    "\n",
    "x = np.array([[1,1,0], [0,0,0], [2,1,2]], int)\n",
    "words = np.array([[1,5,9,4],[3,4,7,1],[4,3,2,5]])\n",
    "arr = word_embedding_forward(words, x)\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) \n",
      "[[  7.  20.  13.  11.]\n",
      " [  4.   5.   9.   7.]\n",
      " [  4.   0.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "dout = np.array([[[1,2,0,1],[3,2,9,1],[1,2,1,1]],\n",
    "                 [[3,9,2,4],[1,9,9,0],[2,0,1,6]],\n",
    "                 [[1,0,1,0],[0,1,0,5],[3,0,0,1]]])\n",
    "\n",
    "arr = word_embedding_backward(dout, words, x)\n",
    "ans = np.array([[  7.,  20.,  13.,  11.],\n",
    "               [  4.,   5.,   9.,   7.],\n",
    "               [  4.,   0.,   1.,   1.]])\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN_step Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "prev_h = (N,H)\n",
    "x = (N, V)\n",
    "W_hh = (H,H)\n",
    "W_xh = (V,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "prev_h = np.random.random((3,5)) # N = 3, H = 5\n",
    "x = np.random.random((3,4)) # N = 3, V = 4\n",
    "W_hh = np.random.random((5,5)) # H = 5\n",
    "W_xh = np.random.random((4,5)) # V = 4, H = 5\n",
    "b = np.random.random((5,)) # H = 5\n",
    "\n",
    "res = rnn_step_forward(prev_h, W_hh, x, W_xh, b) # N = 3, H = 5\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev_h :  1.26218862526e-08\n",
      "dW_hh :  1.26272394557e-08\n",
      "dx :  1.26023289679e-08\n",
      "dW_xh :  1.26237846596e-08\n",
      "db :  1.26068527793e-08\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fprev_h = lambda prev_h: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_hh = lambda W_hh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fx = lambda x: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_xh = lambda W_xh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fb = lambda b: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "\n",
    "dprev_h_num = numerical_gradient_check_multivar(fprev_h, prev_h)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh)\n",
    "dx_num = numerical_gradient_check_multivar(fx, x)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "\n",
    "dW_hh, dW_xh, dprev_h, dx, db = rnn_step_backward(prev_h, W_hh, x, W_xh, b, np.ones_like(res))\n",
    "print \"dprev_h : \", norm_loss(dprev_h, dprev_h_num)\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/layers.py:161: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if h0 != None: # Supply an h0 state.\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "h0 = (N,H)\n",
    "W_hh = (H,H)\n",
    "x = (N,T,D)\n",
    "W_xh = (D,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "N = 3\n",
    "D = 4\n",
    "H = 5\n",
    "T = 1\n",
    "\n",
    "h0 = np.random.random((N,H))\n",
    "W_hh = np.random.random((H,H))\n",
    "x = np.random.random((N,T,D))\n",
    "W_xh = np.random.random((D,H))\n",
    "b = np.random.random((H,))\n",
    "\n",
    "h = rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "\n",
    "print h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_hh :  1.26005078165e-08\n",
      "dW_xh :  1.26025025411e-08\n",
      "dx :  1.25611031233e-08\n",
      "db :  1.25909590028e-08\n",
      "dh0 :  1.26174712937e-08\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fx = lambda x: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_xh = lambda W_xh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_hh = lambda W_hh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fb = lambda b: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fh0 = lambda h0: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "\n",
    "dx_num = numerical_gradient_check_multivar(fx, x)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "dh0_num = numerical_gradient_check_multivar(fh0, h0)\n",
    "\n",
    "dW_hh, dW_xh, dx, db, dh0 = rnn_backward(x, W_xh, W_hh, b, h0, h, np.ones_like(h))\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"db : \", norm_loss(db, db_num)\n",
    "print \"dh0 : \", norm_loss(dh0, dh0_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [[-0.58172089 -0.50182032 -0.41232771 -0.31410098]\n",
      " [ 0.66854692  0.79562378  0.87755553  0.92795967]\n",
      " [ 0.97934501  0.99144213  0.99646691  0.99854353]]\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "next_h = rnn_step_forward(prev_h, Wh, x, Wx, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print next_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "h = (N,H)\n",
    "W_hy = (H,D)\n",
    "b = (D,)\n",
    "\"\"\"\n",
    "h = np.random.random((3,5)) # N = 3, H = 5\n",
    "W_hy = np.random.random((5,7)) # H = 5, D = 7\n",
    "b = np.random.random((7,)) # D = 7\n",
    "\n",
    "res = affine_forward(h, W_hy, b) # N = 3, D = 7\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx :  1.263027982e-08\n",
      "dW_xh :  1.2631957873e-08\n",
      "db :  1.26310626518e-08\n"
     ]
    }
   ],
   "source": [
    "fh = lambda h: affine_forward(h, W_hy, b)\n",
    "fW_hy = lambda W_hy: affine_forward(h, W_hy, b)\n",
    "fb = lambda b: affine_forward(h, W_hy, b)\n",
    "\n",
    "dh_num = numerical_gradient_check_multivar(fh, h)\n",
    "dW_hy_num = numerical_gradient_check_multivar(fW_hy, W_hy)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "\n",
    "dh, dW_hy, db = affine_backward(h, W_hy, b, np.ones_like(res))\n",
    "\n",
    "print \"dx : \", norm_loss(dh, dh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_hy, dW_hy_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer for RNN's #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "h = (N,T,H)\n",
    "W_hy = (H,D)\n",
    "b = (D,)\n",
    "\"\"\"\n",
    "h = np.random.random((3,7,5)) # N = 3, T = 7, H = 5\n",
    "W_hy = np.random.random((5,7)) # H = 5, D = 7\n",
    "b = np.random.random((7,)) # D = 7\n",
    "\n",
    "res = rnn_affine_forward(h, W_hy, b) # N = 3, D = 7\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx :  1.26309993476e-08\n",
      "dW_xh :  1.26313237317e-08\n",
      "db :  1.26310626624e-08\n"
     ]
    }
   ],
   "source": [
    "fh = lambda h: rnn_affine_forward(h, W_hy, b)\n",
    "fW_hy = lambda W_hy: rnn_affine_forward(h, W_hy, b)\n",
    "fb = lambda b: rnn_affine_forward(h, W_hy, b)\n",
    "\n",
    "dh_num = numerical_gradient_check_multivar(fh, h)\n",
    "dW_hy_num = numerical_gradient_check_multivar(fW_hy, W_hy)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "\n",
    "dh, dW_hy, db = rnn_affine_backward(h, W_hy, b, np.ones_like(res))\n",
    "\n",
    "print \"dx : \", norm_loss(dh, dh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_hy, dW_hy_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Layer - One of the most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26099759008e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,4)) # N = 3, D = 4\n",
    "y = np.random.randint(4, size=3) # D = 4, N = 3\n",
    "\n",
    "fx = lambda x: softmax(x, y)[0]\n",
    "\n",
    "loss, dJ = softmax(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Layer - The other most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26399460897e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,5)) # N = 3, D = 5\n",
    "y = np.random.randint(5, size=3) # D = 5, N = 3\n",
    "\n",
    "fx = lambda x: SVM(x, y)[0]\n",
    "\n",
    "loss, dJ = SVM(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax for RNN layer - the important function compatible for RNN's #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26306575262e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,T,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,5,4)) # N = 3, T = 5, D = 4\n",
    "y = np.random.randint(4, size=(3,5)) # D = 4, N = 3\n",
    "\n",
    "fx = lambda x: rnn_softmax(x, y)[0]\n",
    "\n",
    "loss, dJ = rnn_softmax(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I needed regularization on my net so I dropped out of UCLA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000.0\n",
      "mask sum :  36128.0\n",
      "60213.3333333\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,T,D)\n",
    "\"\"\"\n",
    "x = np.ones((30,50,40), float) # N = 30, T = 50, D = 40\n",
    "# by def, 30x50x40 = sum of x = 60,000\n",
    "p = 0.6 # we should see a number around 36,000\n",
    "\n",
    "# Numerical checking on this kind of stuff is sketch since we have randomness in the function\n",
    "print np.sum(x)\n",
    "x,_ = dropout_forward(x, p)\n",
    "print np.sum(x)\n",
    "# These should be relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The entire RNN forward() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.42070749 -0.27279261 -0.11074945  0.05740409  0.22236251]\n",
      "  [-0.39525808 -0.22554661 -0.0409454   0.14649412  0.32397316]\n",
      "  [-0.42305111 -0.24223728 -0.04287027  0.15997045  0.35014525]]\n",
      "\n",
      " [[-0.55857474 -0.39065825 -0.19198182  0.02378408  0.23735671]\n",
      "  [-0.27150199 -0.07088804  0.13562939  0.33099728  0.50158768]\n",
      "  [-0.51014825 -0.30524429 -0.06755202  0.17806392  0.40333043]]]\n"
     ]
    }
   ],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h = rnn_forward(x, Wx, Wh, b, h0)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One pass #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_rnn  :  [ 0.22199317]\n",
      "W_xh  :  [[ 0.87073231]\n",
      " [ 0.20671916]]\n",
      "W_hy  :  [[ 0.91861091  0.48841119]]\n",
      "words  :  [[ 0.61174386  0.76590786]\n",
      " [ 0.51841799  0.2968005 ]]\n",
      "b_affine  :  [ 0.18772123  0.08074127]\n",
      "W_hh  :  [[ 0.7384403]]\n"
     ]
    }
   ],
   "source": [
    "N, D, V, H, T = 1,2,2,1,1\n",
    "\n",
    "model = VanillaRNN(N, D, T, H, V)\n",
    "\n",
    "np.random.seed(5)\n",
    "for key in model.params:\n",
    "    model.params[key] = np.random.random(model.params[key].shape)\n",
    "    print key, \" : \", model.params[key]\n",
    "\n",
    "model.loss(np.array([[1]]), np.array([[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
