{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try out some simple functions for numerical_gradient. #\n",
    "\n",
    "We know the linear equation y = 3x should always return 3. Let's check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99999999989\n"
     ]
    }
   ],
   "source": [
    "def linear(x, slope=3):\n",
    "    return slope*x\n",
    "\n",
    "slope = numerical_gradient_check_scalar(linear, 5)\n",
    "print slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.9999999   0.99999997]\n"
     ]
    }
   ],
   "source": [
    "def multi_quadratic(x):\n",
    "    return x[0]**2 + x[1]\n",
    "arr = np.array([2,2], float)\n",
    "\n",
    "slope = numerical_gradient_check_multivar(multi_quadratic, arr)\n",
    "print slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29.03938293   8.09431076]\n",
      "[[ 8.01086426  3.0040741 ]\n",
      " [ 8.01086426  3.0040741 ]]\n"
     ]
    }
   ],
   "source": [
    "def multi_cubic_field(x):\n",
    "    return np.array([x[0]**3 + x[1]**2, x[0]*2 + x[1]/12])\n",
    "arr = np.array([3,4], dtype=np.float32)\n",
    "\n",
    "def matrix_mult(x, b = np.array([[3,5],[2,1]])):\n",
    "    return x.dot(b)\n",
    "                \n",
    "vector_field = numerical_gradient_check_multivar(multi_cubic_field, arr)\n",
    "print vector_field\n",
    "\n",
    "arr = np.array([[3,4],[1,2]], dtype=np.float32)\n",
    "vector_field = numerical_gradient_check_multivar(matrix_mult, arr)\n",
    "print vector_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fw :  [[ 0.99999997  0.99999997  0.99999997]\n",
      " [ 1.99999995  1.99999995  1.99999995]]\n",
      "fx :  [[ 5.99999985  7.9999998 ]]\n",
      "fb :  [[ 0.99999997  0.99999997  0.99999997]]\n"
     ]
    }
   ],
   "source": [
    "def affine_transform(w, x, b):\n",
    "    return x.dot(w) + b\n",
    "\n",
    "x = np.array([[1,2]], float) # 1 x 2\n",
    "w = np.array([[3,2,1],[1,2,5]], float) # 2 x 3\n",
    "b = np.array([[1,5,7]], float) # 1 x 3\n",
    "\n",
    "fw = lambda w: affine_transform(w,x,b)\n",
    "fx = lambda x: affine_transform(w,x,b)\n",
    "fb = lambda b: affine_transform(w,x,b)\n",
    "\n",
    "vector_field = numerical_gradient_check_multivar(fw, w)\n",
    "print \"fw : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fx, x)\n",
    "print \"fx : \", vector_field\n",
    "vector_field = numerical_gradient_check_multivar(fb, b)\n",
    "print \"fb : \", vector_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word_embedding_forward/backward #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 4) \n",
      "[[[3 4 7 1]\n",
      "  [3 4 7 1]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[1 5 9 4]\n",
      "  [1 5 9 4]\n",
      "  [1 5 9 4]]\n",
      "\n",
      " [[4 3 2 5]\n",
      "  [3 4 7 1]\n",
      "  [4 3 2 5]]]\n"
     ]
    }
   ],
   "source": [
    "# Looks good to me\n",
    "ans = np.array([[[3, 4, 7, 1],\n",
    "                [3, 4, 7, 1],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[1, 5, 9, 4],\n",
    "                [1, 5, 9, 4],\n",
    "                [1, 5, 9, 4]],\n",
    "\n",
    "               [[4, 3, 2, 5],\n",
    "                [3, 4, 7, 1],\n",
    "                [4, 3, 2, 5]]])\n",
    "\n",
    "x = np.array([[1,1,0], [0,0,0], [2,1,2]], int)\n",
    "words = np.array([[1,5,9,4],[3,4,7,1],[4,3,2,5]])\n",
    "arr = word_embedding_forward(words, x)\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) \n",
      "[[  7.  20.  13.  11.]\n",
      " [  4.   5.   9.   7.]\n",
      " [  4.   0.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "dout = np.array([[[1,2,0,1],[3,2,9,1],[1,2,1,1]],\n",
    "                 [[3,9,2,4],[1,9,9,0],[2,0,1,6]],\n",
    "                 [[1,0,1,0],[0,1,0,5],[3,0,0,1]]])\n",
    "\n",
    "arr = word_embedding_backward(dout, words, x)\n",
    "ans = np.array([[  7.,  20.,  13.,  11.],\n",
    "               [  4.,   5.,   9.,   7.],\n",
    "               [  4.,   0.,   1.,   1.]])\n",
    "\n",
    "assert np.array_equal(ans, arr)\n",
    "print arr.shape, \"\\n\", arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN_step Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "prev_h = (N,H)\n",
    "x = (N, V)\n",
    "W_hh = (H,H)\n",
    "W_xh = (V,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "prev_h = np.random.random((3,5)) # N = 3, H = 5\n",
    "x = np.random.random((3,4)) # N = 3, V = 4\n",
    "W_hh = np.random.random((5,5)) # H = 5\n",
    "W_xh = np.random.random((4,5)) # V = 4, H = 5\n",
    "b = np.random.random((5,)) # H = 5\n",
    "\n",
    "res = rnn_step_forward(prev_h, W_hh, x, W_xh, b) # N = 3, H = 5\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev_h :  1.26232714804e-08\n",
      "dW_hh :  1.26096801212e-08\n",
      "dx :  1.26003102835e-08\n",
      "dW_xh :  1.26312391781e-08\n",
      "db :  1.25970797693e-08\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fprev_h = lambda prev_h: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_hh = lambda W_hh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fx = lambda x: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fW_xh = lambda W_xh: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "fb = lambda b: rnn_step_forward(prev_h, W_hh, x, W_xh, b)\n",
    "\n",
    "dprev_h_num = numerical_gradient_check_multivar(fprev_h, prev_h)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh)\n",
    "dx_num = numerical_gradient_check_multivar(fx, x)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "\n",
    "dW_hh, dW_xh, dprev_h, dx, db = rnn_step_backward(prev_h, W_hh, x, W_xh, b, np.ones_like(res))\n",
    "print \"dprev_h : \", norm_loss(dprev_h, dprev_h_num)\n",
    "print \"dW_hh : \", norm_loss(dW_hh, dW_hh_num)\n",
    "print \"dx : \", norm_loss(dx, dx_num)\n",
    "print \"dW_xh : \", norm_loss(dW_xh, dW_xh_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Vanilla RNN Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/layers.py:199: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if h0 != None: # Supply an h0 state.\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "h0 = (N,H)\n",
    "W_hh = (H,H)\n",
    "x = (N,T,D)\n",
    "W_xh = (D,H)\n",
    "b = (H,)\n",
    "\"\"\"\n",
    "N = 3\n",
    "D = 4\n",
    "H = 5\n",
    "T = 1\n",
    "\n",
    "h0 = np.random.random((N,H))\n",
    "W_hh = np.random.random((H,H))\n",
    "x = np.random.random((N,T,D))\n",
    "W_xh = np.random.random((D,H))\n",
    "b = np.random.random((H,))\n",
    "\n",
    "h = rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "\n",
    "print h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before :  [[[ 0.99790491  0.99559902  0.99279909  0.99740809  0.9990061 ]]\n",
      "\n",
      " [[ 0.96434616  0.874288    0.98094905  0.98758858  0.98564568]]\n",
      "\n",
      " [[ 0.98172233  0.98140134  0.98590388  0.98860842  0.98221757]]]\n",
      "current dh :  (3, 1, 5)\n",
      "current db :  [ 0.00542196  0.03658704  0.00391307  0.00103779  0.00352195]\n",
      "h now :  [[[ 0.99790491  0.99559902  0.99279909  0.99740809  0.9990061 ]]\n",
      "\n",
      " [[ 0.96434616  0.874288    0.98094905  0.98758858  0.98564568]]\n",
      "\n",
      " [[ 0.98172233  0.98140134  0.98590388  0.98860842  0.98221757]]]\n",
      "dW_hh :  [[ 0.00527037  0.03555258  0.00383331  0.00101626  0.00343591]\n",
      " [ 0.00495039  0.03346481  0.00370175  0.00097874  0.00330568]\n",
      " [ 0.00533182  0.03597201  0.0038579   0.00102307  0.00346658]\n",
      " [ 0.00536144  0.03616597  0.00387606  0.00102802  0.00348214]\n",
      " [ 0.00534691  0.03605232  0.0038656   0.00102544  0.00346761]] \n",
      "[[ 0.04994755  0.13525028  0.03440349  0.0225354   0.02787352]\n",
      " [ 0.02691275  0.07707871  0.02466777  0.01340069  0.01344126]\n",
      " [ 0.03892711  0.05166291  0.03523018  0.02463556  0.0344265 ]\n",
      " [ 0.06553953  0.14098513  0.05257365  0.03482926  0.04505061]\n",
      " [ 0.04471785  0.14262617  0.03106478  0.01831203  0.01955178]]\n",
      "dW_xh :  [[ 0.00149546  0.00955552  0.00175882  0.0004613   0.00116747]\n",
      " [ 0.00171088  0.01012313  0.00159065  0.00043351  0.00084103]\n",
      " [ 0.00117966  0.00879706  0.00123202  0.00030956  0.0012477 ]\n",
      " [ 0.0028046   0.01847978  0.00155246  0.00042559  0.00141856]] \n",
      "[[ 0.02749883  0.05072751  0.02989007  0.0174597   0.02033351]\n",
      " [ 0.02855417  0.08023252  0.02825606  0.01491292  0.01446831]\n",
      " [ 0.02709559  0.04063026  0.02288983  0.0162929   0.0229367 ]\n",
      " [ 0.05578773  0.16808827  0.03522109  0.02278647  0.02715542]]\n",
      "dx :  [[[ 0.00172016  0.00267672  0.00307899  0.00181892]]\n",
      "\n",
      " [[ 0.01545951  0.02449054  0.02614424  0.01232933]]\n",
      "\n",
      " [[ 0.00812893  0.01307957  0.01290803  0.00704139]]] \n",
      "[[[ 0.00987319  0.01537686  0.02006943  0.01614169]]\n",
      "\n",
      " [[ 0.18689687  0.2869208   0.32124112  0.1735039 ]]\n",
      "\n",
      " [[ 0.06323626  0.09307787  0.09739818  0.08028272]]]\n",
      "db :  [ 0.00542196  0.03658704  0.00391307  0.00103779  0.00352195] \n",
      "[ 0.11044356  0.2812545   0.08008246  0.05249927  0.06573804]\n",
      "dh0 :  [[ 0.00233035  0.00131081  0.00233941  0.00243433  0.00171587]\n",
      " [ 0.01207216  0.00861323  0.01730067  0.01675052  0.00705549]\n",
      " [ 0.00733934  0.00446169  0.0093812   0.00918867  0.0048144 ]] \n",
      "[[ 0.02390302  0.01158826  0.01830502  0.02016017  0.02021417]\n",
      " [ 0.19350719  0.14007656  0.20929707  0.23212109  0.13250766]\n",
      " [ 0.10516426  0.07059869  0.06894939  0.10025325  0.08656879]]\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "fx = lambda x: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_xh = lambda W_xh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fW_hh = lambda W_hh: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fb = lambda b: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "fh0 = lambda h0: rnn_forward(x, W_xh, W_hh, b, h0)\n",
    "\n",
    "dx_num = numerical_gradient_check_multivar(fx, x)\n",
    "dW_xh_num = numerical_gradient_check_multivar(fW_xh, W_xh)\n",
    "dW_hh_num = numerical_gradient_check_multivar(fW_hh, W_hh)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "dh0_num = numerical_gradient_check_multivar(fh0, h0)\n",
    "\n",
    "dW_hh, dW_xh, dx, db, dh0 = rnn_backward_correct(x, W_xh, W_hh, b, h, np.ones_like(h))\n",
    "print \"dW_hh : \", dW_hh, \"\\n\", dW_hh_num\n",
    "print \"dW_xh : \", dW_xh, \"\\n\", dW_xh_num\n",
    "print \"dx : \", dx, \"\\n\", dx_num\n",
    "print \"db : \", db, \"\\n\", db_num\n",
    "print \"dh0 : \", dh0, \"\\n\", dh0_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine Layer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\"\"\"\n",
    "h = (N,H)\n",
    "W_hy = (H,D)\n",
    "b = (D,)\n",
    "\"\"\"\n",
    "h = np.random.random((3,5)) # N = 3, H = 5\n",
    "W_hy = np.random.random((5,7)) # H = 5, D = 7\n",
    "b = np.random.random((7,)) # D = 7\n",
    "\n",
    "res = affine_forward(h, W_hy, b) # N = 3, D = 7\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx :  1.26304513777e-08\n",
      "dW_xh :  1.26317338729e-08\n",
      "db :  1.26310626518e-08\n"
     ]
    }
   ],
   "source": [
    "fh = lambda h: affine_forward(h, W_hy, b)\n",
    "fW_hy = lambda W_hy: affine_forward(h, W_hy, b)\n",
    "fb = lambda b: affine_forward(h, W_hy, b)\n",
    "\n",
    "dh_num = numerical_gradient_check_multivar(fh, h)\n",
    "dW_hy_num = numerical_gradient_check_multivar(fW_hy, W_hy)\n",
    "db_num = numerical_gradient_check_multivar(fb, b)\n",
    "\n",
    "dh, dW_hy, db = affine_backward(h, W_hy, b, np.ones_like(res))\n",
    "\n",
    "print \"dx : \", norm_loss(dh, dh_num)\n",
    "print \"dW_xh : \", norm_loss(dW_hy, dW_hy_num)\n",
    "print \"db : \", norm_loss(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Layer - One of the most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26316073922e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,4)) # N = 3, D = 4\n",
    "y = np.random.randint(4, size=3) # D = 4, N = 3\n",
    "\n",
    "fx = lambda x: softmax(x, y)[0]\n",
    "\n",
    "loss, dJ = softmax(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Layer - The other most important functions in Deep Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ :  1.26399460897e-08\n"
     ]
    }
   ],
   "source": [
    "# Forward and backwards\n",
    "\"\"\"\n",
    "x = (N,D)\n",
    "y = (N,)\n",
    "\"\"\"\n",
    "x = np.random.random((3,5)) # N = 3, D = 5\n",
    "y = np.random.randint(5, size=3) # D = 5, N = 3\n",
    "\n",
    "fx = lambda x: SVM(x, y)[0]\n",
    "\n",
    "loss, dJ = SVM(x, y)\n",
    "dJ_num = numerical_gradient_check_multivar(fx, x)\n",
    "print \"dJ : \", norm_loss(dJ, dJ_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
