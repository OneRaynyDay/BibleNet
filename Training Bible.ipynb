{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools.numerical_gradient import *\n",
    "from models.layers import *\n",
    "from models.networks.vanilla_rnn import *\n",
    "from models.networks.lstm_rnn import *\n",
    "from models.solver.solver import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import h5py\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Constants #\n",
    "#############\n",
    "\n",
    "word_sequence_dest = \"word_sequence.hdf5\"\n",
    "word_mapping_dest = \"word_map.pkl\"\n",
    "idx_mapping_dest = \"idx_map.pkl\"\n",
    "word_dataset_dest = \"word_dataset.hdf5\"\n",
    "cache_model_dest = \"cache_model.pkl\"\n",
    "seq_len = 10\n",
    "\n",
    "delims = ' |\\t|\\r\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data of X_all\n",
    "with h5py.File(word_dataset_dest, 'r') as f:\n",
    "    bible = f[\"bible\"][:]\n",
    "with open(idx_mapping_dest, 'r') as f:\n",
    "    idx_mapping = pickle.load(f)\n",
    "max_idx = np.max(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4094\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters ###\n",
    "N,T = bible.shape\n",
    "num_samples = 256\n",
    "num_words = max_idx+1\n",
    "time_dim = 10\n",
    "hidden_dim = 200\n",
    "word_vec_dim = 200\n",
    "print num_words\n",
    "curPtr = 0\n",
    "\n",
    "rnn = LSTM_RNN(num_samples, num_words, time_dim, hidden_dim, word_vec_dim, l2_lambda=0.0001) # activate regularization\n",
    "solver = Solver({\"learning_rate\" : 5e-3, \"type\" : \"adam\",\n",
    "                 \"beta1\" : 0.9, \"beta2\" : 0.99}) # adagrad/adam can sustain higher learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_prediction(d, loss, seed=860, seq_len=100):\n",
    "    seq = rnn.predict(seed, seq_len=seq_len) # We fed in \"God\" as the first token.\n",
    "    words = [idx_mapping[i] for i in seq]\n",
    "    d[loss] = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch  0 :  83.4880585441\n",
      "loss at epoch  50 :  53.9052425993\n",
      "loss at epoch  100 :  51.0988528886\n",
      "loss at epoch  150 :  45.8932644176\n",
      "loss at epoch  200 :  43.3796779137\n",
      "loss at epoch  250 :  51.7494907532\n",
      "loss at epoch  300 :  40.6747315017\n",
      "loss at epoch  350 :  37.5778213771\n",
      "loss at epoch  400 :  39.7500726394\n",
      "loss at epoch  450 :  42.7446731187\n",
      "loss at epoch  500 :  38.6105970869\n",
      "loss at epoch  550 :  40.6156264635\n",
      "loss at epoch  600 :  50.6368125474\n",
      "loss at epoch  650 :  41.1233254759\n",
      "loss at epoch  700 :  50.6872332572\n",
      "loss at epoch  750 :  40.5473983634\n",
      "loss at epoch  800 :  40.0201072283\n",
      "loss at epoch  850 :  44.4298524655\n",
      "loss at epoch "
     ]
    }
   ],
   "source": [
    "# Run the loss function on the neural network with the parameters: y, h0\n",
    "history = []\n",
    "print_every = 50\n",
    "best_weights = None\n",
    "min_loss = None\n",
    "prediction_history = {}\n",
    "\n",
    "for i in xrange(1000):\n",
    "    if curPtr >= N-num_samples-1:\n",
    "        curPtr = 0\n",
    "    if curPtr == 0:\n",
    "        h0 = np.zeros((num_samples, hidden_dim))\n",
    "    loss, l, h0 = rnn.loss(bible[curPtr:curPtr+num_samples, :], bible[curPtr+1:curPtr+num_samples+1, :], h0)\n",
    "    if min_loss is None or loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = l\n",
    "        populate_prediction(prediction_history, int(loss))\n",
    "    if i % print_every == 0:\n",
    "        print \"loss at epoch \", i, \": \", loss\n",
    "    solver.step(i, loss, l)\n",
    "    curPtr += num_samples\n",
    "    \n",
    "grad_descent_plot = plt.plot(*solver.get_loss_history())\n",
    "plt.setp(grad_descent_plot, 'color', 'r', 'linewidth', 2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"words\", 'w') as f:\n",
    "    f.create_dataset('words', data=l[0][0])\n",
    "with h5py.File(\"W_xh\", 'w') as f:\n",
    "    f.create_dataset(\"W_xh\", data=l[1][0])\n",
    "with h5py.File(\"W_hh\", 'w') as f:\n",
    "    f.create_dataset(\"W_hh\", data=l[2][0])\n",
    "with h5py.File(\"W_hy\", 'w') as f:\n",
    "    f.create_dataset(\"W_hy\", data=l[3][0])\n",
    "with h5py.File(\"b_affine\", 'w') as f:\n",
    "    f.create_dataset(\"b_affine\", data=l[4][0])\n",
    "with h5py.File(\"b_rnn\", 'w') as f:\n",
    "    f.create_dataset(\"b_rnn\", data=l[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only call this cell when you have cached in the states previously!\n",
    "with h5py.File(\"words\", 'r') as f:\n",
    "    rnn.params[\"words\"] = f[\"words\"][:]\n",
    "with h5py.File(\"W_xh\", 'r') as f:\n",
    "    rnn.params[\"W_xh\"] = f[\"W_xh\"][:]\n",
    "with h5py.File(\"W_hh\", 'r') as f:\n",
    "    rnn.params[\"W_hh\"] = f[\"W_hh\"][:]\n",
    "with h5py.File(\"W_hy\", 'r') as f:\n",
    "    rnn.params[\"W_hy\"] = f[\"W_hy\"][:]\n",
    "with h5py.File(\"b_affine\", 'r') as f:\n",
    "    rnn.params[\"b_affine\"] = f[\"b_affine\"][:]\n",
    "with h5py.File(\"b_rnn\", 'r') as f:\n",
    "    rnn.params[\"b_rnn\"] = f[\"b_rnn\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1924, 778, 1251, 289, 1511, 778, 2500, 1924, 2004, 3793, 2091, 169, 289, 1307, 670, 1713, 3527, 289, 1511, 2069, 778, 11, 1924, 778, 3936, 1924, 2288, 670, 3751, 3527, 1088, 289, 1511, 134, 2629, 860, 2614, 758, 670, 3751, 778, 2500, 1924, 2288, 807, 986, 379, 2091, 1852, 725, 2253, 3167, 1511, 359, 3400, 2931, 725, 2836, 670, 1901, 1813, 778, 3488, 517, 3440, 1432, 2850, 303, 289, 534, 289, 722, 3167, 1511, 2614, 3377, 505, 3569, 725, 778, 3716, 1924, 670, 3751, 4005, 2730, 2629, 3226, 3716, 289, 1511, 4005, 1528, 778, 2860, 289, 1475, 3984, 266, 2614, 758, 289, 1511, 533, 2091, 2836, 289, 2964, 2253, 778, 535, 670, 3751, 778, 3137, 1088, 1475, 3558, 929, 2450, 1511, 30, 778, 907, 1924, 1712, 1666, 2660, 238, 3131, 3751, 3199, 2730, 2851, 1924, 315, 289, 3097, 100, 1924, 537, 929, 1985, 303, 289, 1511, 2931, 1178, 289, 2069]\n",
      "of the sword , and the king of Judah began to reign , years . But Elijah , and all the captain of the host of Israel . And Elijah did , and brought him for his hand . And the king of Israel had no given to accepted in Samaria ; and forty footmen be in Jerusalem . Chapter In the LORD shalt surely Speak unto them , saying , shame ; and his hands shall thorns in the <unknown> of . And he smote him from <unknown> , and he saw the pillars , that reigned over his hand , and came to Jerusalem , into Samaria the chariot . And the watchman did that which was twenty and Phinehas the son of Rechab countenance are wrong ? And Elisha smote timber of silver , young men of hunger was with them , and be Therefore , all\n"
     ]
    }
   ],
   "source": [
    "seq = rnn.predict(852, seq_len=150) # We fed in \"God\" as the first token.\n",
    "print seq\n",
    "words = [idx_mapping[i] for i in seq]\n",
    "print \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.585483596433647, 82.890960913599471, 54.927192808938784, 66.616409983764513, 66.985596593904816, 83.026719080666012, 75.92157852744927, 80.322396619154148, 60.641278026358215, 80.582223620789733, 76.727845969987897, 73.239770776682093, 56.15846778702138, 81.556825835363483, 79.077395318832998, 82.623797348416602, 83.284525824614363, 82.346491818891494, 65.432689302988635, 81.956737227702774, 54.988617951563924, 71.154392678780553, 53.269048975791179, 68.979245174516564, 83.166705092144497, 53.238245751484619, 57.52535908187361, 82.762777225135849, 64.117406801376291, 75.10812997404571, 81.672812487360858, 56.506015375014222, 83.383003060934072, 55.268718707100014]\n"
     ]
    }
   ],
   "source": [
    "def print_log(key, prediction_history):\n",
    "    print \"At loss : \", key, \", we have the following phrase : \", prediction_history[key]\n",
    "\n",
    "print prediction_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
